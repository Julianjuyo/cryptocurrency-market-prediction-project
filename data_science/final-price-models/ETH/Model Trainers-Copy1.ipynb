{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e55b9-97c6-4619-b578-7216310b186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  # turn off deprecation warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2aeea6-3418-4f82-ba15-863c2b3e2aa8",
   "metadata": {},
   "source": [
    "# Hour Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea44eee0-e017-44cf-a98b-c758ae7fe931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/latest_hour_final_eth.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a0a8eb-ebf5-47e2-b6f8-3186fbf63da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27cb5d-3c93-4fc4-8f70-0236832c6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.drop(columns=['Unnamed: 0','datetime','DATE'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6dfaab-ca5f-44c9-bd48-3d87d63cb3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Fit the scaler on your dataframe (let's say it's called df)\n",
    "df_normalized = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the normalized data back to a dataframe\n",
    "df_normalized = pd.DataFrame(df_normalized, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43afe18c-7dfc-4da9-9260-e9d44df91dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the DataFrame to a NumPy array\n",
    "raw_data = df_normalized.values\n",
    "close = df_normalized['close'].values\n",
    "close = close.reshape((len(close),1))\n",
    "# Display the NumPy array\n",
    "print(type(raw_data))\n",
    "print(raw_data.shape)\n",
    "print(close.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3cff0c-9fe4-45d8-a4dc-96009de11ced",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efe0c9-3bd6-489e-9608-572dce576afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prep Data')\n",
    "num_train_samples = int(0.6 * len(raw_data))\n",
    "num_val_samples = int(0.25 * len(raw_data))\n",
    "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
    "print(\"num_train_samples:\", num_train_samples)\n",
    "print(\"num_val_samples:\", num_val_samples)\n",
    "print(\"num_test_samples:\", num_test_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e78bd5f-8f25-4c4d-9857-bb4a27721192",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 Hour Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acc9c6-20f6-48a4-9631-dd81768b4e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 1\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 2\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb9771-740c-44b4-81d0-9a4cfa6d6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_1_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72347df-9e3d-46d6-a7da-4854746c53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3063d13-8d58-43f6-9897-6e48fedf1378",
   "metadata": {},
   "source": [
    "## 3 Hour Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd639b8-7064-4dd7-999a-374975a45ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 3\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 2\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9547dd7-b27d-4e1f-9d3d-7f848da1a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_3_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4a006-7436-4c8e-9f81-52ccb51783c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3177a-54cf-4c6a-9236-4e90c511e345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a13219-1f8a-4d6e-9ca9-0383eea7fcf1",
   "metadata": {},
   "source": [
    "## 6 Hours Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f0156-982e-48b7-aa51-e6809084cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 6\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 2\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f797b34-2958-48bc-a823-d3231681f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_6_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2e552-ca7a-44a7-bd39-7e4a614db26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a8e01-0bf4-43d0-b79f-3e18ebc469fb",
   "metadata": {},
   "source": [
    "## 12 Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e92388-cd15-4021-bca8-fda2227cbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 12\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 3\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffd0c9-71ef-4f7b-81f7-8f3cb6aa1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_12_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3e4ea-ee1c-4719-b1b9-f031d26667fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6612e6-5c88-425c-9a57-3df86ab2147d",
   "metadata": {},
   "source": [
    "## 1 Day Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aea1e-da66-497f-9600-19f52bf9210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 24\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 2\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24595b-0ddb-47ab-b966-df8a68182c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_24_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1cf77-7f11-4eab-b21f-4c73891f569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ca862-5a65-4cee-869f-c16dbb1d53f8",
   "metadata": {},
   "source": [
    "## 3 Days Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cbfb5-a5bc-4620-8bac-31e482ea46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 3*24\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 4\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 5 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29cabe-cc01-4563-9891-987aedf85651",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_3d_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d92bd-e96c-40bf-a32e-9c30ca24e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365aac71-427f-4e66-9237-2abe6977f56c",
   "metadata": {},
   "source": [
    "## 7 Days Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa2fa9-365d-4c46-b26b-2ef10aa84668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 7*24\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 6\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 5 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45870c8-3164-465a-9757-88bd3a2462df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_7d_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eae5b2-016f-4e2e-9e3d-e9826043cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bbb882-0ec8-4a8f-b627-fed8a742f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../../data/latest_hour_final_eth.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825a78b-f2d9-46c3-8ebb-b9748781a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52b856e-54a9-4ed6-8d7e-8553b0879e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
