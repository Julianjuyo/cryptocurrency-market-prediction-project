{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43acc9c6-20f6-48a4-9631-dd81768b4e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 1\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 2\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3063d13-8d58-43f6-9897-6e48fedf1378",
   "metadata": {},
   "source": [
    "## 3 Hour Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd639b8-7064-4dd7-999a-374975a45ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 3\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 2\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9547dd7-b27d-4e1f-9d3d-7f848da1a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_3_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4a006-7436-4c8e-9f81-52ccb51783c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3177a-54cf-4c6a-9236-4e90c511e345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a13219-1f8a-4d6e-9ca9-0383eea7fcf1",
   "metadata": {},
   "source": [
    "## 6 Hours Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f0156-982e-48b7-aa51-e6809084cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 6\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 2\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f797b34-2958-48bc-a823-d3231681f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_6_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2e552-ca7a-44a7-bd39-7e4a614db26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a8e01-0bf4-43d0-b79f-3e18ebc469fb",
   "metadata": {},
   "source": [
    "## 12 Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e92388-cd15-4021-bca8-fda2227cbe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 12\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 3\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffd0c9-71ef-4f7b-81f7-8f3cb6aa1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_12_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3e4ea-ee1c-4719-b1b9-f031d26667fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6612e6-5c88-425c-9a57-3df86ab2147d",
   "metadata": {},
   "source": [
    "## 1 Day Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aea1e-da66-497f-9600-19f52bf9210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 24\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 2\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 4 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24595b-0ddb-47ab-b966-df8a68182c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_24_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1cf77-7f11-4eab-b21f-4c73891f569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ca862-5a65-4cee-869f-c16dbb1d53f8",
   "metadata": {},
   "source": [
    "## 3 Days Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cbfb5-a5bc-4620-8bac-31e482ea46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 3*24\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 4\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 5 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29cabe-cc01-4563-9891-987aedf85651",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_3d_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6d92bd-e96c-40bf-a32e-9c30ca24e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365aac71-427f-4e66-9237-2abe6977f56c",
   "metadata": {},
   "source": [
    "## 7 Days Ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa2fa9-365d-4c46-b26b-2ef10aa84668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Parameters\n",
    "\n",
    "# delay: time in future that will be predicted\n",
    "delay = 7*24\n",
    "\n",
    "\n",
    "# sampling rate: period between timesteps within the sequence\n",
    "# Sequence with rate=1 : t1,t2...tn\n",
    "# Sequence with rate=3 : t1,t3...tn*3\n",
    "sampling_rate = 6\n",
    "\n",
    "# sequence length: sequence lenght of each sample \n",
    "sequence_length = 5 * 24\n",
    "\n",
    "\n",
    "# sequence_stride: period between sequences\n",
    "# First sequence starts at t0\n",
    "# Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "sequence_stride = 1\n",
    "\n",
    "#batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "#If None, the data will not be batched (the dataset will yield individual samples).\n",
    "# Huge impact in performance.\n",
    "# Tip, should be multiple of 8\n",
    "batch_size = 32\n",
    "\n",
    "# Understanding our parameters\n",
    "msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "print(msg)\n",
    "\n",
    "train_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=0,\n",
    "                    end_index=num_train_samples)\n",
    "\n",
    "print(\"Done Train\")\n",
    "\n",
    "val_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples,\n",
    "                    end_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Validation\")      \n",
    "\n",
    "test_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                    raw_data[:-delay],\n",
    "                    targets=close[delay:],\n",
    "                    sampling_rate=sampling_rate,\n",
    "                    sequence_stride=sequence_stride,\n",
    "                    sequence_length=sequence_length,\n",
    "                    shuffle=False,\n",
    "                    seed=33,\n",
    "                    batch_size=batch_size,\n",
    "                    start_index=num_train_samples + num_val_samples)\n",
    "\n",
    "print(\"Done Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45870c8-3164-465a-9757-88bd3a2462df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
    "x = layers.LSTM(32)(inputs) \n",
    "\n",
    "\n",
    "outputs = layers.Dense(1)(x) #simoidal #con n clases neurnas y funcion softmax\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"models/lstm\",\n",
    "   save_best_only=True) \n",
    "]\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[callbacks, early_stopping])\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('lstm_model_7d_ahead.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eae5b2-016f-4e2e-9e3d-e9826043cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MSE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MSE\")\n",
    "plt.title(\"Training and validation MSE\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "loss = history.history[\"mae\"]\n",
    "val_loss = history.history[\"val_mae\"]\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
    "plt.title(\"Training and validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
