{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn\n",
    "# pip install keras\n",
    "# pip install tensorflow\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_columns', 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"prueba.csv\")\n",
    "\n",
    "df.drop(columns=['id','date_time_utc','unix_time'], inplace=True)\n",
    "\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_preparation(df):\n",
    "\n",
    "    # drop the datetime column\n",
    "    df_reduced = df.drop(columns=['datetime'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler on your dataframe (let's say it's called df)\n",
    "    df_normalized = scaler.fit_transform(df_reduced)\n",
    "\n",
    "    # Convert the normalized data back to a dataframe\n",
    "    df_normalized = pd.DataFrame(df_normalized, columns=df_reduced.columns)\n",
    "\n",
    "    # Convert the DataFrame to a NumPy array\n",
    "    raw_data = df_normalized.values\n",
    "    close = df_normalized['close_price'].values\n",
    "    close = close.reshape((len(close),1))\n",
    "\n",
    "    # Display the NumPy array\n",
    "    print(type(raw_data))\n",
    "    print(raw_data.shape)\n",
    "    print(close.shape)\n",
    "\n",
    "    return raw_data, close\n",
    "\n",
    "\n",
    "\n",
    "def predict(df,model_path,delay,sampling_rate,sequence_length):\n",
    "\n",
    "    raw_data, close = data_preparation(df)\n",
    "\n",
    "    # Set Parameters\n",
    "\n",
    "    # sequence_stride: period between sequences\n",
    "    # First sequence starts at t0\n",
    "    # Second sequence will start at t1 with sequence_stride=1 or at t5 with sequence_stride=5\n",
    "    sequence_stride = 1\n",
    "\n",
    "    #batch_size: Number of timeseries samples in each batch (except maybe the last one). \n",
    "    #If None, the data will not be batched (the dataset will yield individual samples).\n",
    "    # Huge impact in performance.\n",
    "    # Tip, should be multiple of 8\n",
    "    batch_size = 32\n",
    "\n",
    "    # Understanding our parameters\n",
    "    msg = f\"The timeseries will consist of batches containing {batch_size} sequences of {sequence_length} samples.\"\n",
    "\n",
    "    msg += f\"\\nFinally our target is {delay} timesteps in the future, and will have data from {sequence_length * sampling_rate} timesteps in the past\"\n",
    "    print(msg)\n",
    "\n",
    "    keras_dataset = keras.preprocessing.timeseries_dataset_from_array(\n",
    "                        raw_data[:-delay],\n",
    "                        targets=close[delay:],\n",
    "                        sampling_rate=sampling_rate,\n",
    "                        sequence_stride=sequence_stride,\n",
    "                        sequence_length=sequence_length,\n",
    "                        shuffle=False, # Shouldn't the shuffle be set to 0?\n",
    "                        seed=33,\n",
    "                        batch_size=batch_size,\n",
    "                        start_index=0)\n",
    "    \n",
    "    # Load the model from the .h5 file\n",
    "    modelo =  load_model(model_path)\n",
    "\n",
    "    test_pred = modelo.predict(keras_dataset)\n",
    "\n",
    "    # Assume 'y_normalized' contains the predicted values for the 'target' column in normalized form\n",
    "    y_min = df['close_price'].min()\n",
    "    y_max = df['close_price'].max()\n",
    "\n",
    "    test_pred = test_pred * (y_max - y_min) + y_min\n",
    "\n",
    "    predicted_prices = pd.DataFrame(test_pred, columns=['close_price'])\n",
    "\n",
    "    return predicted_prices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
